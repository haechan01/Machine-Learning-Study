{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# popular_videos\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def get_liked_videos(max_results=200):\n",
    "    videos = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(videos) < max_results:\n",
    "        request = youtube.videos().list(\n",
    "            part=\"snippet,contentDetails,statistics,topicDetails\",\n",
    "            myRating=\"like\",\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "\n",
    "        response = request.execute()\n",
    "        videos.extend(response.get(\"items\", []))\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return videos[:max_results]\n",
    "\n",
    "def get_random_videos_details(video_ids):\n",
    "    videos = []\n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        request = youtube.videos().list(\n",
    "            part=\"snippet,contentDetails,statistics,topicDetails\",\n",
    "            id=','.join(video_ids[i:i+50])\n",
    "        )\n",
    "        response = request.execute()\n",
    "        videos.extend(response.get(\"items\", []))\n",
    "    return videos\n",
    "\n",
    "def get_random_videos(max_results=200, regions=['US', 'KR']):\n",
    "    video_ids = []\n",
    "\n",
    "    for region in regions:\n",
    "        while len(video_ids) < max_results:\n",
    "            request = youtube.videos().list(\n",
    "                part=\"snippet\",\n",
    "                chart=\"mostPopular\",\n",
    "                regionCode=region,\n",
    "                maxResults=50\n",
    "            )\n",
    "\n",
    "            response = request.execute()\n",
    "            video_ids.extend([item['id'] for item in response.get(\"items\", [])])\n",
    "\n",
    "            if len(video_ids) >= max_results:\n",
    "                break\n",
    "\n",
    "    return get_random_videos_details(video_ids[:max_results])\n",
    "\n",
    "# Get 200 liked videos\n",
    "liked_videos = get_liked_videos(max_results=200)\n",
    "\n",
    "# Get 200 random videos from YouTube (from US and Korea)\n",
    "random_videos = get_random_videos(max_results=200, regions=['US', 'KR'])\n",
    "\n",
    "# Create DataFrame to store video information\n",
    "def create_dataframe(video_items, category):\n",
    "    data = []\n",
    "    for item in video_items:\n",
    "        snippet = item.get(\"snippet\", {})\n",
    "        content_details = item.get(\"contentDetails\", {})\n",
    "        statistics = item.get(\"statistics\", {})\n",
    "        topic_details = item.get(\"topicDetails\", {})\n",
    "\n",
    "        video_info = {\n",
    "            \"like\": category,\n",
    "            \"title\": snippet.get(\"title\", \"N/A\"),\n",
    "            \"description\": snippet.get(\"description\", \"N/A\"),\n",
    "            \"tags\": snippet.get(\"tags\", \"N/A\"),\n",
    "            \"category_id\": snippet.get(\"categoryId\", \"N/A\"),\n",
    "            \"duration\": content_details.get(\"duration\", \"N/A\"),\n",
    "            \"view_count\": int(statistics.get(\"viewCount\", 0)),\n",
    "            \"like_count\": int(statistics.get(\"likeCount\", 0)),\n",
    "            \"comment_count\": int(statistics.get(\"commentCount\", 0)),\n",
    "            \"topic_categories\": topic_details.get(\"topicCategories\", \"N/A\"),\n",
    "            \"language\": snippet.get(\"defaultAudioLanguage\", snippet.get(\"defaultLanguage\", \"N/A\"))\n",
    "        }\n",
    "        data.append(video_info)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Create DataFrames for liked and random videos\n",
    "liked_videos_df = create_dataframe(liked_videos, \"Liked\")\n",
    "random_videos_df = create_dataframe(random_videos, \"Random\")\n",
    "\n",
    "# Combine both DataFrames\n",
    "combined_df = pd.concat([liked_videos_df, random_videos_df], ignore_index=True)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "combined_df.to_csv(\"combined_videos.csv\", index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(combined_df.head())\n",
    "\n",
    "# Summarize the dataset\n",
    "print(\"\\nDataset Summary:\")\n",
    "print(combined_df.describe())\n",
    "\n",
    "# Bar graph to compare number of liked and random videos\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x=\"like\", data=combined_df)\n",
    "plt.title(\"Number of Videos: Liked vs Random\")\n",
    "plt.xlabel(\"Like\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Compare views of liked and random videos\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=\"like\", y=\"view_count\", data=combined_df)\n",
    "plt.title(\"View Count Comparison: Liked vs Random Videos\")\n",
    "plt.xlabel(\"Like\")\n",
    "plt.ylabel(\"View Count\")\n",
    "plt.yscale(\"log\")  # Use log scale to handle wide range of view counts\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your DataFrame after processing the upload date\n",
    "# If not already done, process the upload date features:\n",
    "\n",
    "# Convert 'upload_date' to datetime object\n",
    "df['upload_date'] = pd.to_datetime(df['upload_date'], errors='coerce')\n",
    "\n",
    "# Extract year, month, day, weekday, and hour\n",
    "df['upload_year'] = df['upload_date'].dt.year\n",
    "df['upload_month'] = df['upload_date'].dt.month\n",
    "df['upload_day'] = df['upload_date'].dt.day\n",
    "df['upload_weekday'] = df['upload_date'].dt.weekday  # Monday=0, Sunday=6\n",
    "df['upload_hour'] = df['upload_date'].dt.hour\n",
    "\n",
    "# Optionally, drop the original 'upload_date' column if not needed\n",
    "# df = df.drop(columns=['upload_date'])\n",
    "\n",
    "# Define the features and target variable\n",
    "features = [\n",
    "    'view_count', 'like_count', 'comment_count', 'duration_seconds', \n",
    "    'category_id', 'upload_year', 'upload_month', 'upload_day', 'upload_weekday', 'upload_hour'\n",
    "]\n",
    "target = 'like'\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing: Feature scaling for numerical features and one-hot encoding for categorical features\n",
    "\n",
    "# Decide which date features are numerical and which are categorical\n",
    "numerical_features = [\n",
    "    'view_count', 'like_count', 'comment_count', 'duration_seconds'\n",
    "]\n",
    "categorical_features = ['category_id']\n",
    "\n",
    "# Define the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a pipeline with preprocessing and logistic regression\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Train the logistic regression model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Neural Network\n",
    "\n",
    "The fourth model we will use is a **Neural Network**. Neural Networks are powerful machine learning models capable of capturing complex, non-linear relationships in the data. Given the diverse set of features we have, including numerical metadata and textual features, a Neural Network can help discover intricate patterns that may not be easily captured by simpler models like Logistic Regression or Naive Bayes.\n",
    "\n",
    "Neural Networks consist of layers of interconnected nodes (neurons). The basic structure involves an **input layer**, one or more **hidden layers**, and an **output layer**. Each neuron in a layer takes inputs, applies a weight to each input, sums them, and then applies an **activation function** to introduce non-linearity. Mathematically, for a neuron $j$ in a hidden layer, the output can be represented as:\n",
    "\n",
    "$\n",
    "Z_j = f\\left( \\sum_{i=1}^n w_{ij}X_i + b_j \\right)\n",
    "$\n",
    "\n",
    "where $X_i$ represents the inputs, $w_{ij}$ are the weights associated with each input, $b_j$ is the bias term, and $f$ is the activation function. Common activation functions include **ReLU** (Rectified Linear Unit), **sigmoid**, and **tanh**. In our model, we will use the ReLU activation function for hidden layers to help the network learn non-linear relationships efficiently.\n",
    "\n",
    "The **output layer** uses a sigmoid activation function to output a probability score for the \"liked\" class, as this is a binary classification problem. The output can be represented as:\n",
    "\n",
    "$\n",
    "P(y = 1 | X) = \\sigma(Z) = \\frac{1}{1 + e^{-Z}}\n",
    "$\n",
    "\n",
    "where $\\sigma(Z)$ is the sigmoid function. The network will be trained using **backpropagation**, which involves calculating the error at the output, propagating it back through the network, and adjusting the weights using **gradient descent** to minimize the error.\n",
    "\n",
    "We are using a Neural Network model because it allows us to capture complex relationships within the data, especially when combining both numerical and textual features. The flexibility of Neural Networks makes them suitable for this task, as they can adapt to the various input types and find underlying interactions between features that simpler models might miss.\n",
    "\n",
    "#### Pseudocode for Neural Network model\n",
    "\n",
    "1. **Define the structure of the neural network** (input layer, hidden layers, output layer).\n",
    "\n",
    "2. **Initialize weights and biases** for all neurons to small random values.\n",
    "\n",
    "3. **For each epoch** (iteration over training data):\n",
    "\n",
    "   - **Forward Propagation**:\n",
    "\n",
    "     - Compute the linear combination of inputs and weights for each neuron in the hidden layer:\n",
    "       $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$, where $A^{[0]}$ is the input data.\n",
    "     - Apply activation functions (e.g., ReLU for hidden layers):\n",
    "       $A^{[l]} = f(Z^{[l]})$.\n",
    "     - Compute the output using a sigmoid activation function in the output layer.\n",
    "\n",
    "   - **Calculate the Loss**:\n",
    "\n",
    "     - Use binary cross-entropy loss to measure the error between predictions and actual values.\n",
    "\n",
    "   - **Backward Propagation**:\n",
    "\n",
    "     - Compute the gradients of the loss with respect to weights and biases for each layer.\n",
    "     - Apply the chain rule to propagate errors back through the network and adjust weights accordingly.\n",
    "\n",
    "   - **Gradient Descent Update**:\n",
    "\n",
    "     - Update weights and biases using a learning rate to minimize the loss.\n",
    "\n",
    "4. **Train the model** using the above steps until convergence or maximum epochs are reached.\n",
    "\n",
    "5. **Make predictions** on the test set by performing forward propagation with the learned weights.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network model is built using a sequential structure consisting of an input layer, two hidden layers, and an output layer. The input layer has 128 neurons, followed by a second hidden layer with 64 neurons, both using Leaky ReLU as the activation function to address the problem of dead neurons that can occur with standard ReLU. The output layer uses a sigmoid activation function for binary classification, providing output values between 0 and 1. The model is compiled using the Adam optimizer, which helps in efficient training, and binary cross-entropy loss to evaluate the error during the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6: Train Neural Network Model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Combine Logistic Regression numeric data and Naive Bayes text data for Neural Network\n",
    "X_combined_nn_train = hstack([X_train_lr, X_train_nb])\n",
    "X_combined_nn_test = hstack([X_test_lr, X_test_nb])\n",
    "\n",
    "# Scale the combined features for neural network\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_train_nn_scaled = scaler.fit_transform(X_combined_nn_train)\n",
    "X_test_nn_scaled = scaler.transform(X_combined_nn_test)\n",
    "\n",
    "# Initialize neural network model\n",
    "neural_network_model = Sequential()\n",
    "neural_network_model.add(Dense(128, input_dim=X_train_nn_scaled.shape[1], activation='relu'))\n",
    "neural_network_model.add(Dropout(0.5))\n",
    "neural_network_model.add(Dense(64, activation='relu'))\n",
    "neural_network_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the neural network\n",
    "neural_network_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the neural network\n",
    "neural_network_model.fit(X_train_nn_scaled, y_train_lr, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the neural network on the test set\n",
    "test_loss, test_accuracy = neural_network_model.evaluate(X_test_nn_scaled, y_test_lr)\n",
    "\n",
    "print(f\"Neural Network Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the target variable\n",
    "target = 'like'\n",
    "\n",
    "# Combine title and description into a single text feature\n",
    "df['combined_text'] = df['title'].astype(str) + ' ' + df['description'].astype(str)\n",
    "\n",
    "# Convert combined text to feature matrix using CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "text_features = vectorizer.fit_transform(df['combined_text']).toarray()\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numerical_features = ['view_count', 'like_ratio', 'duration_seconds']\n",
    "categorical_features = pd.get_dummies(df['category_id']).values\n",
    "numerical_data = df[numerical_features].fillna(0).values\n",
    "\n",
    "# Combine numerical, categorical, and text features\n",
    "X = np.hstack((numerical_data, categorical_features, text_features))\n",
    "y = df[target].values\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling for numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train[:, :len(numerical_features)] = scaler.fit_transform(X_train[:, :len(numerical_features)])\n",
    "X_test[:, :len(numerical_features)] = scaler.transform(X_test[:, :len(numerical_features)])\n",
    "\n",
    "# Convert target variable to categorical\n",
    "y_train_cat = to_categorical(y_train, num_classes=2)\n",
    "y_test_cat = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "# Create Neural Network model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train_cat, epochs=10, batch_size=32, validation_data=(X_test, y_test_cat))\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# NLP with title and description\n",
    "# Tokenize titles and descriptions and count word frequency\n",
    "df['combined_text'] = df['combined_text'].astype(str)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def tokenize_and_filter(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "    return words\n",
    "\n",
    "# Apply tokenization\n",
    "df['tokenized_combined'] = df['combined_text'].apply(tokenize_and_filter)\n",
    "\n",
    "# Count word frequencies for 'like' and 'not like' groups\n",
    "liked_words = df[df['like'] == 1]['tokenized_combined'].sum()\n",
    "unliked_words = df[df['like'] == 0]['tokenized_combined'].sum()\n",
    "\n",
    "liked_word_freq = Counter(liked_words)\n",
    "unliked_word_freq = Counter(unliked_words)\n",
    "\n",
    "# Get the most common words in each group\n",
    "most_common_liked = liked_word_freq.most_common(10)\n",
    "most_common_unliked = unliked_word_freq.most_common(10)\n",
    "\n",
    "print(\"\\nMost Common Words in Liked Videos:\")\n",
    "print(most_common_liked)\n",
    "print(\"\\nMost Common Words in Not Liked Videos:\")\n",
    "print(most_common_unliked)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
